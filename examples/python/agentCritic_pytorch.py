#!/usr/bin/env python3

# M. Kempka, T.Sternal, M.Wydmuch, Z.Boztoprak
# January 2021

import itertools as it
import os
from collections import deque
from random import sample
from time import sleep, time

import numpy as np
import skimage.color
import skimage.transform
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import trange

import vizdoom as vzd

import matplotlib.pyplot as plt


# Q-learning settings
episodes = 5
learning_rate = 0.00025
discount_factor = 0.99
replay_memory_size = 1000
num_train_epochs = 5
learning_steps_per_epoch = 200
target_net_update_steps = 100
batch_size = 8
# Training regime
test_episodes_per_epoch = 10

# Other parameters
frames_per_action = 3
resolution = (120, 90)
resolution_buffer = (120, 90*3)

episodes_to_watch = 20

save_model = True
load = False
skip_learning = False
watch = False
player_skip = frames_per_action
# Configuration file path
model_savefolder_actor = "F:/SIiUM3/ViZDoom/model1.0/actor"
model_savefolder_critic = "F:/SIiUM3/ViZDoom/model1.0/critic"


if len(tf.config.experimental.list_physical_devices("GPU")) > 0:
    print("GPU available")
    DEVICE = "/gpu:0"
else:
    print("No GPU available")
    DEVICE = "/cpu:0"


def preprocess(img):
    img = np.mean(img, axis = 0)
    img = skimage.transform.resize(img, resolution)
    img = img.astype(np.float32)
    # img = np.expand_dims(img, axis=-1)

    return img


class AgentCriticActor(nn.Module):
    def __init__(self, num_actions):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=6, stride=3, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(8, 12, kernel_size=5, stride=2, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(12, 16, kernel_size=3, stride=2, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )

        self.conv4 = nn.Sequential(
            nn.Conv2d(16, 24, kernel_size=3, stride=2, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )


        self.advantage_fc = nn.Sequential(
            nn.Linear(96, 64), nn.Softmax()
        )

    def call(self, x):
        x1 = self.conv1(x)
        x1 = self.conv2(x1)
        x1 = self.conv3(x1)
        x1 = self.conv4(x1)
        x = self.flatten(x1)
        x = self.advantage(x)

        return x
    
class AgentCriticCritic(Model):
    def __init__(self, num_actions):
        super().__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 8, kernel_size=6, stride=4, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(8, 12, kernel_size=5, stride=2, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(12, 16, kernel_size=3, stride=2, bias=False),
            nn.BatchNorm2d(8),
            nn.ReLU(),
        )


        self.state_fc = nn.Sequential(nn.Linear(self.conv3.shape, 1), nn.ReLU(), nn.Linear(64, 1))


    def call(self, x):
        x1 = self.conv1(x)
        x1 = self.conv2(x1)
        x1 = self.conv3(x1)
    
        x = self.state_fc(x1)

        return x
    
class AgentCritic:
    def __init__(self, num_actions):
        self.num_actions = num_actions
        self.gamma = 0.60    # discount rate
        self.learning_rate = 0.001

        if load:
            print("Loading model actor from: ", model_savefolder_actor, 
                  "\n Loading model critic from: ",model_savefolder_critic)
            self.actor = tf.keras.models.load_model(model_savefolder_actor,  compile=False)
            self.critic = tf.keras.models.load_model(model_savefolder_critic,  compile=False)

        else:
            self.actor = AgentCriticActor(self.num_actions)
            self.critic = AgentCriticCritic(self.num_actions)


        # self.actor.optimizer = Adam(learning_rate =0.00001)
        # self.critic.optimizer = Adam(learning_rate =0.00005)
        self.actor.compile(loss = 'mse', optimizer = Adam(learning_rate =0.00001),  metrics=['accuracy'])
        self.critic.compile(loss = 'mse', optimizer = Adam(learning_rate =0.00005), metrics=['accuracy'])

    def choose_action(self, state, variables = None):
        """
        Compute the action to take in the current state, basing on policy returned by the network.

        Note: To pick action according to the probability generated by the network
        """
        # state = tf.convert_to_tensor([state], dtype=tf.float32)
        state = np.array([state], dtype=np.float32)


        action_probability = self.actor.predict(state, verbose = 0)

        ids = np.arange(len(action_probability[0]))
        chosen_action = np.random.choice(ids, p = action_probability[0])
        return chosen_action

  

    def train(self, state, actions, reward, next_state, dones):


        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:
            tape1.watch(self.actor.trainable_variables)
            tape2.watch(self.critic.trainable_variables)
            state = tf.convert_to_tensor(state, dtype=tf.float32)
            next_state = tf.convert_to_tensor(next_state, dtype=tf.float32)
            probs = self.actor(state)
            value = self.critic(state)
            next_value = self.critic(next_state)

            t1 = 1 - np.array(dones)[:, np.newaxis]
            t2 = t1 * self.gamma
            t3 = t2 * next_value
            value_target = reward[:, np.newaxis] + t3 - value
            actions = np.array(actions, dtype=int)[:, np.newaxis]
            # probs_action = np.zeros((batch_size,1), dtype=int)
            test = np.array(probs)
            result = np.take_along_axis(test, actions, axis = 1)

            # for idx, p in enumerate(probs):
            #     a = actions[idx][0]
            #     probs_action[idx] = p[a]
            # probs_action = tf.convert_to_tensor(probs_action, dtype=tf.float32)
            
            log_prob = tf.math.log(result)
            # log_prob = tf.math.log(probs[actions])

            loss_actor = -log_prob*value_target
            loss_critic = value_target**2

        loss_actor = tf.convert_to_tensor(loss_actor, dtype=tf.float32)
        loss_critic = tf.convert_to_tensor(loss_critic, dtype=tf.float32)
        gradient_actor = tape1.gradient(loss_actor, self.actor.trainable_variables)
        gradient_critic = tape2.gradient(loss_critic, self.critic.trainable_variables)

        self.actor.optimizer.apply_gradients(zip(gradient_actor, self.actor.trainable_variables))
        self.critic.optimizer.apply_gradients(zip(gradient_critic, self.critic.trainable_variables))

def extractDigits(*argv):
    if len(argv) == 1:
        return list(map(lambda x: [x], argv[0]))

    return list(map(lambda x, y: [x, y], argv[0], argv[1]))

def split_tuple(samples):
    samples = np.array(samples, dtype=object)
    screen_buf = tf.stack(samples[:, 0])
    actions = samples[:, 1]
    rewards = tf.stack(samples[:, 2])
    next_screen_buf = tf.stack(samples[:, 3])
    dones = tf.stack(samples[:, 4])
    return screen_buf, actions, rewards, next_screen_buf, dones

def get_samples(memory):
    if len(memory) < batch_size:
        sample_size = len(memory)
    else:
        sample_size = batch_size

    return sample(memory, sample_size)

def update_buffer(img, framebuffer):
    axis = -1
    framebuffer = np.concatenate([img, framebuffer], axis = axis)
    return framebuffer

def get_reward(action_reward, variables, old_variables):
    reward = .0
    if old_variables[1] > variables[1] and old_variables[2] == variables[2]:
        reward -=.0
    else:
        reward += 0.5

    return reward + action_reward
    
def run(agent, game, actions, train_scores, framebuffer, framebuffer_next, replay_memory):
    
    state = game.get_state()
    screen_buf = preprocess(state.screen_buffer)
    framebuffer = update_buffer(screen_buf, framebuffer[:,:-90])
    action = agent.choose_action(framebuffer)
    
    reward_action = game.make_action(actions[action], frames_per_action)

    reward = reward_action
    done = game.is_episode_finished()

    if not done:
        next_screen_buf = preprocess(game.get_state().screen_buffer)
        framebuffer_next = update_buffer(next_screen_buf, framebuffer_next[:,:-90])

    else:
        next_screen_buf = tf.zeros(shape=screen_buf.shape)
        framebuffer_next = update_buffer(next_screen_buf, framebuffer_next[:,:-90])

    replay_memory.append((framebuffer, action, reward, framebuffer_next, done))
    
    if len(replay_memory) >= batch_size:    
        screen_buf, actions, rewards, next_screen_buf, dones = split_tuple(get_samples(replay_memory))

        agent.train(screen_buf, actions, rewards, next_screen_buf, dones)

    if game.is_player_dead() or done:
        train_scores.append(game.get_total_reward())

    return train_scores, action, reward, framebuffer

